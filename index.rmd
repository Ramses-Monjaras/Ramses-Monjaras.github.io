---
title: "Video Game Data"
author: "Ramses Monjaras"
date: "`r Sys.Date()`"
output: html_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(boot)
library(dplyr)
library(tidyr)
library(ggplot2)
library(caret)
library(lattice)
library(rpart)
library(rpart.plot)
library(purrr)
library(datasets)
library(e1071)
library(cluster)
library(factoextra)
library(dbscan)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("game_dataset.csv")
```
  Video games are very popular, that thousands of them get released throughout the years on different platforms. I started to wonder which platforms have the best games and active player base. You can find this data set in the [Kaggle](https://www.kaggle.com/datasets/zafarali27/game-data) website. In the data set the game ID, name, genre, release year and platform are all categorical variables. The global sales, user rating, and active players are all quantitative variables and they are also all ordinal. This data set seems to be cleaned from the start, so I didn't touch anything. Below is what my data set look like.
```{r}
  df[1:10,]
```
# Part 1
For one of my quantitative variable, I am going to take the user rating to give the summary statistics. Below is the five point summary which is min = 1.0, Q1 = 3.3, med = 5.5, Q3 = 7.6, and max = 10. The mean is 5.476 and the standard deviation is 2.562662. Below is also a histogram of the user score.
```{r}
summary(df$User_Rating)
sd(df$User_Rating)
```
```{r}
ggplot(df, aes(x = User_Rating)) + geom_histogram() + labs(title = "User Ratings") + geom_histogram(color = 'black')
```
 
  As you can see in the histogram, x-axis is the user rating and the y-axis is the rating count. It looks like people gave a lot of games a 4 rating which tells me that there is not a lot of games that people will rate a high score. The graph look noisy and all over the place.The graph also doesn't look like it is skewed. 
```{r}
ggplot(data = df, aes(x = Platform, y = User_Rating)) + geom_boxplot(color = 'black') + labs(title = "Box Plot of User Ratings by Platform", x = "Platform", y = "User Rating")
```
```{r}
five_point_summary <- df |>
  group_by(Platform) |>
  summarise(
    Min = min(User_Rating, na.rm = TRUE),
    Q1 = quantile(User_Rating, 0.25, na.rm = TRUE),
    Median = median(User_Rating, na.rm = TRUE),
    Q3 = quantile(User_Rating, 0.75, na.rm = TRUE),
    Max = max(User_Rating, na.rm = TRUE),
    Mean = mean(User_Rating, na.rm = TRUE)
  )

print(five_point_summary)
```

As you can see in the box plot above, the x-axis is the different platforms and the y-axis is the user ratings. It looks like there are no outliers in the box plots. What I find interesting is the switch games having a lower user rating on average compared to the other platforms. This side-by-side plot also shows how many high rated games are on each platform. It looks like Xbox games has a higher user rating compared to the other platforms. Next, I will take a categorical variable to create a frequency and a relative frequency table. Below is the frequency table and relative frequency.
```{r}
df |> count(Genre) |>
  mutate(relative_frequency = n / sum(n))
```
The total count of the genre is 2,000 and the total count of the relative frequency is 1. What I found interesting is that there are more racing games compared to all the different genres.
```{r}
two_way_table <- df |>
  count(Platform, Genre) |>
  pivot_wider(names_from = Genre, values_from = n, values_fill = list(n = 0))
print(two_way_table)
```
In the two-way table above, the table shows the different amount of genres that each platform has. I find interesting that there is a lot of Pc racing games but less amount of shooter games in the PC platform. The possible relationship of these two variables is that, if you are interested in a specific genre of games, then you can look at this table and see which platform has the most of that genre.

In conclusion, what I found interesting that Switch games have a lower user rating on average and Xbox games has the highest rating on average. I also find that there are more racing games among each platform compared to all the different types of genres. I expected to be more shooter games then racing games.

# Part 2

```{r}
model <- lm(Active_Players ~ User_Rating, data = df)
summary(model)


plot(df$User_Rating, df$Active_Players, xlab="User Rating", ylab="Active Players" )

abline(model, col="red")
```

As you can see this simple y ~ x regression is very all over the place and the regression line seems to be in a straight line. This tells me that if a game has a high rating, that doesn't mean people will keep playing the game. I also include statistics above that shows the five point summary of the regression. Below will be the multiple regression.

```{r}
model <- lm(Active_Players ~ User_Rating + Global_Sales + Release_Year, data = df)

summary(model)

par(mfrow = c(1, 3))
plot(df$User_Rating, df$Active_Players, main = "Active Players vs User Rating", xlab = "User Rating", ylab = "Active Players", pch = 16, col = "blue")
abline(lm(Active_Players ~ User_Rating, data = df), col = "red")

plot(df$Global_Sales, df$Active_Players, main = "Active Players vs Global Sales", xlab = "Global Sales", ylab = "Active Players", pch = 16, col = "green")
abline(lm(Active_Players~ Global_Sales, data = df), col = "red")

plot(df$Release_Year, df$Active_Players, main = "Active Players vs Release Year", xlab = "Release Year", ylab = "Active Players", pch = 16, col = "purple")
abline(lm(Active_Players~ Release_Year, data = df), col = "red")

par(mfrow = c(2, 2))
plot(model)

coefficients <- coef(model)
print(coefficients)

r_squared <- summary(model)$r.squared
adj_r_squared <- summary(model)$adj.r.squared
print(paste("R-squared:", r_squared))
print(paste("Adjusted R-squared:", adj_r_squared))

p_values <- summary(model)$coefficients[, 4]
print(p_values)
```

As you can see above, the multiple regression is still noisy. In the Active Player vs. User Rating, the regression line seems to be negative, Active Players vs. Global Sales seems to be positive, and Active Players vs. Release Year is also positive. What I find interesting is how Active Players seems to go down when a game has a higher user rating. I also added a statistic to show the five point summary of the multiple regression.

```{r}
model <- lm(User_Rating ~ Active_Players + Global_Sales + Active_Players:Global_Sales, data = df)

summary(model)

ggplot(df, aes(x = Active_Players, y = User_Rating, color = Global_Sales)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Scatter plot of User Rating vs Active Players, colored by Global Sales",
       x = "Active Players", y = "User Rating", color = "Global Sales") +
  theme_minimal()

interaction.plot(df$Active_Players, df$Global_Sales, df$User_Rating,
                 xlab = "Active Players", ylab = "User Rating", trace.label = "Global Sales",
                 main = "Interaction Plot of Active Players and Global Sales on User Rating")

par(mfrow = c(2, 2))
plot(model)

coefficients <- coef(model)
print(coefficients)

r_squared <- summary(model)$r.squared
adj_r_squared <- summary(model)$adj.r.squared
print(paste("R-squared:", r_squared))
print(paste("Adjusted R-squared:", adj_r_squared))

p_values <- summary(model)$coefficients[, 4]
print(p_values)
```

In the multiple regression with the interaction, I decide to change it up a little and Use user rating as y to see if anything became different. In the graph, it appears that the user rating of 6 appears to have lower active players. This graph also show the Global sales, the lighter the color, the higher the sales. AS you can see I also added the interaction plot that shows the interaction between active players and global sales, there is also statistical data I added above.

```{r}
model <- nls(Global_Sales ~ a * exp(b * User_Rating), data = df, start = list(a = 1, b = 0.1))

summary(model)

plot(df$User_Rating, df$Global_Sales, main = "Non-linear Regression: Exponential Growth",
     xlab = "User Rating", ylab = "Global Sales", pch = 16, col = "blue")

predict_fun <- function(x) {
  predict(model, newdata = data.frame(User_Rating = x))
}

curve(predict_fun(x), add = TRUE, col = "red", lwd = 2)

legend("topleft", legend = c("Data Points", "Fitted Curve"),
       col = c("blue", "red"), pch = c(16, NA), lty = c(NA, 1))

fitted_values <- predict(model)
residuals <- residuals(model)

par(mfrow = c(1, 2)) 
plot(fitted_values, residuals, main = "Residuals vs Fitted",
     xlab = "Fitted Values", ylab = "Residuals", pch = 16, col = "blue")
abline(h = 0, col = "red", lty = 2)

qqnorm(residuals, main = "Normal Q-Q Plot of Residuals", pch = 16, col = "blue")
qqline(residuals, col = "red")

parameters <- coef(model)
print(parameters)

resid_se <- summary(model)$sigma
print(paste("Residual Standard Error:", resid_se))

conf_int <- confint(model)
print(conf_int)
```

Above is the non-linear regression. It looks like there is not much of a growth happening on both the exponential growth and the residuals vs fitted graphs. The Q-Q plot has a increasing regression line which I found interesting. 

#Part 3

```{r}
tree <- rpart(Platform ~ Active_Players + User_Rating, data = df, control = rpart.control(minslpit = 2, minbucket = 1, cp = 0.01))
tree <- rpart(Platform ~., data =df[, c("Platform", "User_Rating", "Active_Players")])


rpart.plot(tree, extra = 2,
           type = 4,
           fallen.leaves = TRUE,
           box.palette="white",
           branch.lty = 3,
           shadow.col = "gray")


pred <- predict(tree, type = "class")
head(pred)

predict(tree) |> head()

confusionMatrix(table(predicted =pred, actual = df$Platform))

summary(tree)


```


Above is the simple decision tree and I'm using the game platform as my categorical variable. I also have a confusion matrix that shows the prediction of the game platform. I believe that the active players is the most important variable.

```{r}

train_control <- trainControl(
  method = "cv",        
  number = 10,          
  savePredictions = TRUE,
  classProbs = TRUE,    
  summaryFunction = multiClassSummary  
)


set.seed(42)
tree_model_cv <- train(
  Platform ~ Active_Players + User_Rating,
  data = df,
  method = "rpart",
  trControl = train_control,
  tuneLength = 5,       # Try 5 different complexity parameters
  metric = "Accuracy"   # Optimization metric
)


print(tree_model_cv)


cv_results <- tree_model_cv$results
print(cv_results)


print(paste("Optimal cp:", tree_model_cv$bestTune$cp))


cv_confusion <- confusionMatrix(tree_model_cv$pred$pred, tree_model_cv$pred$obs)
print(cv_confusion)


ggplot(as.data.frame(cv_confusion$table), 
       aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Cross-Validated Confusion Matrix", 
       x = "Actual Platform", 
       y = "Predicted Platform") +
  theme_minimal()


var_imp <- varImp(tree_model_cv)
plot(var_imp, main = "Variable Importance from Cross-Validated Model")


rpart.plot(
  tree_model_cv$finalModel,
  type = 4,
  extra = 104,
  box.palette = "Blues",
  shadow.col = "gray",
  nn = TRUE,
  main = "Optimized Decision Tree (After Cross-Validation)"
)
```


I repeated my decision tree above but this time I'm using the k-fold validation as a cross validation technique. Using this technique seems to have a higher accuracy compared to the first model. The model does seem to over fit because the decision tree is very complex. I also notice that the confusion matrix shows all the prediction of all the game platforms.

```{r}
set.seed(123)
train_index <- createDataPartition(df$Platform, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]




tree_model <- rpart(Platform ~ Active_Players + User_Rating,
                    data = train_data,
                    method = "class")


rpart.plot(tree_model, main = "Decision Tree for Game Platform Classification",
           box.palette = "BuGn", shadow.col = "gray", nn = TRUE)


tree_pred <- predict(tree_model, test_data, type = "class")





importance <- tree_model$variable.importance
importance_df <- data.frame(Variable = names(importance), Importance = importance)
importance_df <- importance_df[order(-importance_df$Importance), ]


ggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Variable Importance in Decision Tree",
       x = "Variables",
       y = "Importance") +
  theme_minimal()


cat("The most important variables for platform classification are:\n")
print(head(importance_df, 3))
cat("\nFrom the variable importance analysis, we can see that", 
    importance_df$Variable[1], "is the most significant predictor of game platform,",
    "followed by", importance_df$Variable[2], "and", importance_df$Variable[3], 
    ". This suggests that these features have the strongest influence on determining",
    "which platform a game might belong to.")




train_data$Platform <- as.factor(train_data$Platform)
test_data$Platform <- as.factor(test_data$Platform)


svm_formula <- Platform ~ Active_Players + User_Rating
svm_train_x <- model.matrix(svm_formula, data = train_data)[,-1]  
svm_train_y <- train_data$platform

svm_test_x <- model.matrix(svm_formula, data = test_data)[,-1]
svm_test_y <- test_data$platform


train_data$platform <- as.factor(train_data$Platform)
test_data$platform <- as.factor(test_data$Platform)



svm_train_x <- model.matrix(Platform ~ Active_Players + User_Rating, 
                           data = train_data)

svm_test_x <- model.matrix(Platform ~ Active_Players + User_Rating,
                          data = test_data)


svm_train_y <- train_data$Platform
svm_test_y <- test_data$Platform


preProc <- preProcess(svm_train_x, method = c("center", "scale"))
svm_train_x_scaled <- predict(preProc, svm_train_x)
svm_test_x_scaled <- predict(preProc, svm_test_x)


svm_model <- svm(x = svm_train_x_scaled,
                 y = svm_train_y,
                 type = "C-classification",
                 kernel = "radial",
                 probability = TRUE,
                 scale = FALSE)  

svm_pred <- predict(svm_model, svm_test_x_scaled)

conf_matrix <- confusionMatrix(svm_pred, svm_test_y)
print(conf_matrix)

```

Above I use the same categorical variable of the game platform but this time I use SVM to train the variable. In this model the accuracy is higher than the first model but less then the second. The Active Players is still an important variable in the decision tree.

#Part 4

```{r}
cl <- kmeans(df[,c("Global_Sales", "User_Rating")],2)
plot(df[,c("Global_Sales","User_Rating")], col = cl$cluster)# pch =df$Platform)
points(cl$cluster, col = 1:3, pch = 8, cex = 2)

fviz_nbclust(x = df[,c("Global_Sales","User_Rating")],FUNcluster = kmeans, method = 'wss' )
```


In part 4, I picked the game's platform as my categorical variable. As you can see above, the table shows the k means clustering with 2 different clusters. As you can see above there is also a table the shows the optimal number of clusters, it shows that 2 is the optimal number.  

```{r}
res <- dbscan(df[,c("Global_Sales", "User_Rating")], eps = 3, minPts = 5)
plot(df[,c("Global_Sales", "User_Rating")], col = res$cluster)
points(df[res$cluster == 0,c("Global_Sales", "User_Rating")], pch = 3, col = "grey")

clplot(df[,c("Global_Sales", "User_Rating")],res)

x <- scale(df[,c("Global_Sales", "User_Rating")])

res <- dbscan(x, eps = 0.2)
clplot(x,res)
```

Above, I use the DBSCAN for my categorical variable. The main difference I can see is that the clustering has two different groups but the dbscan is all one group. There isn't any outliers from clustering and the dbscan that I've seen. From the two different techniques I think it is not fair to compare them because the dbscan is looking for patterns and clustering is looking for which data is closer to each other.